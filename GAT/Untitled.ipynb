{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da46b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [33 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\torch_scatter\n",
      "  copying torch_scatter\\placeholder.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\n",
      "  copying torch_scatter\\scatter.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\n",
      "  copying torch_scatter\\segment_coo.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\n",
      "  copying torch_scatter\\segment_csr.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\n",
      "  copying torch_scatter\\testing.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\n",
      "  copying torch_scatter\\utils.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\n",
      "  copying torch_scatter\\__init__.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\n",
      "  creating build\\lib.win-amd64-cpython-39\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\logsumexp.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\softmax.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\std.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\\composite\n",
      "  copying torch_scatter\\composite\\__init__.py -> build\\lib.win-amd64-cpython-39\\torch_scatter\\composite\n",
      "  running egg_info\n",
      "  writing torch_scatter.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_scatter.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_scatter.egg-info\\requires.txt\n",
      "  writing top-level names to torch_scatter.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*' found under directory 'test'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'torch_scatter.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  C:\\Users\\shiva\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\cpp_extension.py:359: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_scatter._scatter_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-scatter\n",
      "ERROR: Could not build wheels for torch-scatter, which is required to install pyproject.toml-based projects\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [56 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\add.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\bandwidth.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\cat.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\coalesce.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\convert.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\diag.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\eye.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\index_select.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\masked_select.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\matmul.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\metis.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\mul.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\narrow.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\permute.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\reduce.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\rw.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\saint.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\sample.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\select.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\spadd.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\spmm.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\spspmm.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\storage.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\tensor.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\testing.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\transpose.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\typing.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\utils.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  copying torch_sparse\\__init__.py -> build\\lib.win-amd64-cpython-39\\torch_sparse\n",
      "  running egg_info\n",
      "  writing torch_sparse.egg-info\\PKG-INFO\n",
      "  writing dependency_links to torch_sparse.egg-info\\dependency_links.txt\n",
      "  writing requirements to torch_sparse.egg-info\\requires.txt\n",
      "  writing top-level names to torch_sparse.egg-info\\top_level.txt\n",
      "  reading manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\css'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\html'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\tests'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\examples'\n",
      "  warning: no previously-included files matching '*' found under directory 'third_party\\parallel-hashmap\\benchmark'\n",
      "  warning: no previously-included files matching '*' found under directory 'test'\n",
      "  warning: no previously-included files matching '*' found under directory 'benchmark'\n",
      "  adding license file 'LICENSE'\n",
      "  writing manifest file 'torch_sparse.egg-info\\SOURCES.txt'\n",
      "  running build_ext\n",
      "  C:\\Users\\shiva\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\utils\\cpp_extension.py:359: UserWarning: Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n",
      "    warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "  building 'torch_sparse._convert_cpu' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for torch-sparse\n",
      "ERROR: Could not build wheels for torch-sparse, which is required to install pyproject.toml-based projects\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\shiva\\appdata\\roaming\\python\\python39\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9f3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3876223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple PyTorch Implementation of the Graph Attention layer.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GATLayer, self).__init__()\n",
    "      \n",
    "    def forward(self, input, adj):\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4bec6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "in_features = 5\n",
    "out_features = 2\n",
    "nb_nodes = 3\n",
    "\n",
    "W = nn.Parameter(torch.zeros(size=(in_features, out_features))) #xavier paramiter inizializator\n",
    "nn.init.xavier_uniform_(W.data, gain=1.414)\n",
    "\n",
    "input = torch.rand(nb_nodes,in_features) \n",
    "\n",
    "\n",
    "# linear transformation\n",
    "h = torch.mm(input, W)\n",
    "N = h.size()[0]\n",
    "\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217c588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "a = nn.Parameter(torch.zeros(size=(2*out_features, 1))) #xavier paramiter inizializator\n",
    "nn.init.xavier_uniform_(a.data, gain=1.414)\n",
    "print(a.shape)\n",
    "\n",
    "leakyrelu = nn.LeakyReLU(0.2)  # LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "998d73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b34080b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = leakyrelu(torch.matmul(a_input, a).squeeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bf57fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 4]) torch.Size([4, 1])\n",
      "\n",
      "torch.Size([3, 3, 1])\n",
      "\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(a_input.shape,a.shape)\n",
    "print(\"\")\n",
    "print(torch.matmul(a_input,a).shape)\n",
    "print(\"\")\n",
    "print(torch.matmul(a_input,a).squeeze(2).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd5df397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "adj = torch.randint(2, (3, 3))\n",
    "\n",
    "zero_vec  = -9e15*torch.ones_like(e)\n",
    "print(zero_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f2dc8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0],\n",
      "        [0, 1, 1],\n",
      "        [0, 1, 1]]) \n",
      " tensor([[-0.2695, -0.1108, -0.2437],\n",
      "        [-0.2416, -0.0829, -0.2157],\n",
      "        [-0.2717, -0.1130, -0.2459]], grad_fn=<LeakyReluBackward0>) \n",
      " tensor([[-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15],\n",
      "        [-9.0000e+15, -9.0000e+15, -9.0000e+15]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-9.0000e+15, -1.1082e-01, -9.0000e+15],\n",
       "        [-9.0000e+15, -8.2878e-02, -2.1574e-01],\n",
       "        [-9.0000e+15, -1.1304e-01, -2.4590e-01]], grad_fn=<WhereBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = torch.where(adj > 0, e, zero_vec)\n",
    "print(adj,\"\\n\",e,\"\\n\",zero_vec)\n",
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b9c1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = F.softmax(attention, dim=1)\n",
    "h_prime   = torch.matmul(attention, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aa2bb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.dropout       = dropout        # drop prob = 0.6\n",
    "        self.in_features   = in_features    # \n",
    "        self.out_features  = out_features   # \n",
    "        self.alpha         = alpha          # LeakyReLU with negative input slope, alpha = 0.2\n",
    "        self.concat        = concat         # conacat = True for all layers except the output layer.\n",
    "\n",
    "        \n",
    "        # Xavier Initialization of Weights\n",
    "        # Alternatively use weights_init to apply weights of choice \n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        \n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        \n",
    "        # LeakyReLU\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "        \n",
    "    def forward(self, input, adj):\n",
    "        # Linear Transformation\n",
    "        h = torch.mm(input, self.W) # matrix multiplication\n",
    "        N = h.size()[0]\n",
    "        print(N)\n",
    "\n",
    "        # Attention Mechanism\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1).view(N, -1, 2 * self.out_features)\n",
    "        e       = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        # Masked Attention\n",
    "        zero_vec  = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        \n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime   = torch.matmul(attention, h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "813fc305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes in Cora: 7\n",
      "Number of Node Features in Cora: 1433\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "name_data = 'Cora'\n",
    "dataset = Planetoid(root= '/tmp/' + name_data, name = name_data)\n",
    "dataset.transform = T.NormalizeFeatures()\n",
    "\n",
    "print(f\"Number of Classes in {name_data}:\", dataset.num_classes)\n",
    "print(f\"Number of Node Features in {name_data}:\", dataset.num_node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcbf64d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9491, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.7355, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5445, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.6143, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.5020, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GAT, self).__init__()\n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "        \n",
    "        \n",
    "        self.conv1 = GATConv(dataset.num_features, self.hid, heads=self.in_head, dropout=0.6)\n",
    "        self.conv2 = GATConv(self.hid*self.in_head, dataset.num_classes, concat=False,\n",
    "                             heads=self.out_head, dropout=0.6)\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "                \n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = \"cpu\"\n",
    "\n",
    "model = GAT().to(device)\n",
    "data = dataset[0].to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    if epoch%200 == 0:\n",
    "        print(loss)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ce6379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8130\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
